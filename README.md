# ScriptedVLA - Vision-Language-Action Model based on Qwen VLM and Flow Matching

A clear and easy-to-understand VLA (Vision-Language-Action) training and inference project, based on Qwen open-source small VLM model and Flow Matching action head. No tricks, no over-encapsulation, no over-modularization design, aiming to provide a clear and easy-to-understand VLA model implementation. Script is all you need.  --- author: @Benny Lu (hitlxg@gmail.com)

---

# ScriptedVLA - åŸºäºQwen VLMå’ŒFlow Matchingçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹

ä¸€ä¸ªæ¸…æ™°æ˜“æ‡‚çš„VLAï¼ˆVision-Language-Actionï¼‰è®­ç»ƒå’Œæ¨ç†é¡¹ç›®ï¼ŒåŸºäºQwenå¼€æºå°VLMæ¨¡å‹å’ŒFlow MatchingåŠ¨ä½œå¤´ã€‚ä¸ç©å¥—è·¯ï¼Œä¸åšè¿‡åº¦å°è£…ï¼Œä¸åšè¿‡åº¦æ¨¡å—åŒ–è®¾è®¡ï¼Œæ—¨åœ¨æä¾›ä¸€ä¸ªæ¸…æ™°ã€æ˜“äºç†è§£çš„VLAæ¨¡å‹å®ç°ã€‚Script is all you need.  --- author: @Benny Lu (hitlxg@gmail.com)

---

## English Version

### Project Features

- ğŸ¯ **Simple and Clear**: Clean code structure with detailed comments, no over-encapsulation, suitable for learning and research
- ğŸ”§ **Easy Configuration**: YAML configuration file for convenient hyperparameter adjustment
- ğŸš€ **Complete Pipeline**: Includes data loading, model training, inference, and other complete functionalities
- ğŸ“¦ **Modern Tools**: Uses uv for virtual environment management
- ğŸ§© **Modular Design**: Independent components, easy to extend and modify
- ğŸ¤– **LeRobot Support**: Native support for LeRobot dataset format (v2.1 and v3.0), compatible with HuggingFace open-source datasets
- ğŸ”„ **Unified Interface**: Unified dictionary format input, automatic state dimension handling, simplified usage
- ğŸ§ª **Complete Testing**: Includes comprehensive test suite to ensure code quality

### Project Structure

```
ScriptedVLA/
â”œâ”€â”€ config.yaml                  # Configuration file
â”œâ”€â”€ pyproject.toml               # Project dependencies (uv)
â”œâ”€â”€ train.py                     # Training script (supports LeRobot datasets)
â”œâ”€â”€ train_public_datasets.py     # Training script for public datasets
â”œâ”€â”€ inference.py                 # Inference script
â”œâ”€â”€ create_dummy_data.py         # Create test data
â”œâ”€â”€ dataset_statistics.py        # Dataset statistics and filtering tools
â”œâ”€â”€ download_model.py            # Model download script
â”œâ”€â”€ analyze_state_dimensions.py  # State dimension analysis tool
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ VLM_EVALUATION.md            # VLM capability evaluation guide
â”œâ”€â”€ test/                        # Test directory
â”‚   â”œâ”€â”€ test_vla_qwen_groot.py   # VLA model tests
â”‚   â”œâ”€â”€ test_vlm.py              # VLM model tests
â”‚   â”œâ”€â”€ test_action_head.py      # Action head tests
â”‚   â”œâ”€â”€ test_lerobot_training.py # LeRobot training tests
â”‚   â”œâ”€â”€ test_lerobot_dataset_loader.py # LeRobot dataset loader tests
â”‚   â”œâ”€â”€ test_training.py         # Training pipeline tests
â”‚   â”œâ”€â”€ test_inference.py        # Inference tests
â”‚   â””â”€â”€ evaluate_vlm_capabilities.py # VLM capability evaluation script
â””â”€â”€ src/
    â””â”€â”€ ScriptedVLA/            # Python package (uv standard structure)
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ model/               # Model definitions
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ vlm.py          # Qwen VLM model
        â”‚   â”œâ”€â”€ action_head.py  # Flow Matching action head
        â”‚   â””â”€â”€ vla_qwen_groot.py  # Qwen-GR00T VLA model
        â”œâ”€â”€ data/                # Data processing
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ dataset.py      # Custom dataset classes
        â”‚   â”œâ”€â”€ download_datasets.py # Dataset download utilities
        â”‚   â”œâ”€â”€ libero_dataset.py   # LIBERO dataset adapter
        â”‚   â”œâ”€â”€ act_dataset.py      # ACT dataset adapter
        â”‚   â””â”€â”€ lerobot_dataset_adapter.py # LeRobot dataset adapter
        â””â”€â”€ utils/               # Utility functions
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ config.py       # Configuration loading
            â”œâ”€â”€ logger.py       # Logging utilities
            â””â”€â”€ normalization.py # State normalization utilities
```

### Quick Start

#### 1. Environment Setup

Create a virtual environment and install dependencies using uv:

```bash
# Install uv (if not already installed)
pip install uv

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
uv pip install -e .
```

#### 2. Prepare Data

The project primarily supports **LeRobot dataset format** (v2.1 and v3.0), which is the default and recommended approach.

**Using LeRobot Datasets:**

```bash
# Train with LeRobot dataset (default uses ./dataset/libero_object)
python train.py --config config.yaml

# Specify LeRobot dataset path
python train.py --config config.yaml --dataset_path ./dataset/libero_object

# Train with HuggingFace LeRobot dataset
# Set in config.yaml:
# dataset:
#   local_path: null
#   # Or use HF dataset name directly
```

**Supported LeRobot Datasets:**
- `lerobot/pusht`: PushT dataset
- `k1000dai/libero-object-smolvla`: LIBERO Object dataset (LeRobot format)
- Other HuggingFace LeRobot format datasets

#### 3. Configure Model

Edit `config.yaml` to adjust model and training parameters:

```yaml
model:
  vlm:
    model_name: "Qwen/Qwen2-VL-2B-Instruct"  # Recommended model
    image_size: 448
    freeze_vlm: true  # Freeze VLM parameters
  
  action_head:
    hidden_dim: 1536  # Match VLM output dimension
    num_layers: 6
    num_heads: 12
    action_dim: 7  # Action dimension
    action_horizon: 50  # Action sequence length

dataset:
  local_path: "./dataset/libero_object"
  action_horizon: 50
  image_size: 224
  image_keys:
    - "observation.images.wrist_image"
  state_key: "observation.state"
  action_dim: 7

training:
  batch_size: 8
  num_epochs: 100
  learning_rate: 1e-4
  ...
```

#### 4. Train Model

```bash
# Train with LeRobot dataset (default)
python train.py --config config.yaml

# Specify dataset path
python train.py --config config.yaml --dataset_path ./dataset/libero_object

# Set training steps and save interval
python train.py --config config.yaml --max_steps 20000 --save_steps 5000

# Resume from checkpoint
python train.py --config config.yaml --resume ./checkpoints/checkpoint_epoch_50.pt
```

#### 5. Download Models

```bash
# Download Qwen2-VL-2B-Instruct model
python download_model.py --model Qwen/Qwen2-VL-2B-Instruct

# Or download other models
python download_model.py --model Qwen/Qwen-VL-Chat
```

#### 6. Evaluate VLM Capabilities

```bash
# Run complete robot capability evaluation
python test/evaluate_vlm_capabilities.py --model Qwen/Qwen2-VL-2B-Instruct

# Use model from config file
python test/evaluate_vlm_capabilities.py --config config.yaml
```

The evaluation includes:
- **Object Recognition**: Identify objects, colors, quantities in images
- **Spatial Perception**: Understand positional relationships, distances, directions
- **Causal Reasoning**: Action-result reasoning, scene understanding, logical reasoning

#### 7. Inference

```bash
python inference.py \
    --config config.yaml \
    --checkpoint ./checkpoints/best_model.pt \
    --image path/to/image.jpg \
    --text "Pick up the object"
```

### Model Architecture

#### VLM Module (Qwen)
- Based on Qwen-VL model for vision-language understanding
- Processes image and text inputs, outputs fused features
- Supports freezing VLM parameters to speed up training

#### Action Head (Flow Matching)
- Based on Flow Matching architecture
- Predicts robot action sequences (action horizon) from VLM features
- Contains multi-layer Transformer (DiT Block) and positional encoding
- Supports action chunking (predicting future action sequences)
- **Timestep Encoding**: Encodes timesteps into embedding vectors via `TimestepEncoder`
- **Adaptive Normalization**: Supports `AdaLayerNorm`, dynamically adjusting normalization parameters via timestep embeddings

#### Complete VLA Model
- Combines VLM and action head
- Optional cross-attention mechanism for enhanced feature fusion
- End-to-end training

### Configuration Guide

#### Model Configuration
- `vlm.model_name`: Qwen model name
- `vlm.image_size`: Input image size (recommended: 448)
- `vlm.freeze_vlm`: Whether to freeze VLM parameters
- `action_head.hidden_dim`: Transformer hidden dimension (should match VLM output)
- `action_head.num_layers`: Number of Transformer layers
- `action_head.num_heads`: Number of attention heads
- `action_head.action_dim`: Action dimension (e.g., 7D: x, y, z, roll, pitch, yaw, gripper)
- `action_head.action_horizon`: Action sequence length (chunk size)

#### Training Configuration
- `batch_size`: Batch size
- `learning_rate`: Learning rate
- `num_epochs`: Number of training epochs
- `max_steps`: Maximum training steps
- `optimizer`: Optimizer configuration
- `scheduler`: Learning rate scheduler configuration

#### Dataset Configuration
- `dataset.local_path`: Local dataset path (LeRobot format)
- `dataset.action_horizon`: Action sequence length
- `dataset.image_size`: Image size
- `dataset.image_keys`: Image key names from dataset (e.g., `["observation.images.wrist_image"]`)
- `dataset.state_key`: State key name from dataset
- `dataset.action_dim`: Action dimension
- `dataset.task_description.use_batch_task`: Get task description from batch (recommended)
- `dataset.task_description.use_tasks_jsonl`: Get task description from tasks.jsonl (fallback)

### LeRobot Dataset Support

LeRobot is an open-source robot learning dataset format on HuggingFace, supporting v2.1 and v3.0 versions. The project uses LeRobot datasets by default for training.

**LeRobot Dataset Features:**
- Supports Parquet format storage (v3.0) and HDF5 format (v2.1)
- Automatic version detection and compatibility handling
- Supports action chunking (action sequence prediction)
- Includes task descriptions and metadata

**Usage Example:**
```bash
# Train with LeRobot dataset (default)
python train.py --config config.yaml --dataset_path ./dataset/libero_object

# Use HuggingFace dataset
# In config.yaml, set dataset.local_path to null or use HF dataset name
```

### Package Structure

This project uses a standard Python package structure, compatible with uv and modern Python package management tools:

- **Package Name**: `ScriptedVLA` (in `src/ScriptedVLA/` directory)
- **Import Style**: `from ScriptedVLA.model import ...` (after installation)
- **Installation**: `uv pip install -e .` (editable mode, code changes take effect immediately)

**Advantages:**
- âœ… Complies with PEP 517/518 standards
- âœ… Supports uv, pip, poetry and other modern package managers
- âœ… Easy code organization and modularization
- âœ… Can be imported as a library by other projects

### Development Guide

#### Adding New Features
1. **Model Extensions**: Add new modules in `src/ScriptedVLA/model/`
2. **Data Processing**: Add data augmentation or new datasets in `src/ScriptedVLA/data/`
3. **Utilities**: Add helper functions in `src/ScriptedVLA/utils/`

#### Import Style
All script files use the following import style:
```python
from ScriptedVLA.model import QwenGR00TVLAModel
from ScriptedVLA.data import VLADataset, LeRobotDatasetAdapter
from ScriptedVLA.utils import load_config, setup_logger, Normalizer
```

#### Code Standards
- Use type hints
- Add docstrings
- Keep code clean and clear

### Testing

The project includes a comprehensive test suite:

```bash
# Run all tests
pytest test/

# Run specific tests
pytest test/test_vla_qwen_groot.py
pytest test/test_lerobot_training.py
pytest test/test_training.py
pytest test/test_inference.py
```

### Common Questions

**Q: How to adjust action dimension?**  
A: Modify the `action_head.action_dim` parameter in `config.yaml`.

**Q: How to freeze VLM parameters?**  
A: Set `vlm.freeze_vlm: true` in the configuration file.

**Q: What is AdaLayerNorm? How to use it?**  
A: `AdaLayerNorm` is an adaptive layer normalization that dynamically adjusts normalization parameters via timestep embeddings (temb). This can improve Flow Matching performance by allowing the model to adapt normalization based on timestep information.

**Q: What is timestep embedding (temb)?**  
A: Timestep embedding encodes timesteps in Flow Matching into vector representations, implemented via `TimestepEncoder`. When using `ada_norm`, timestep embeddings are passed to `DiTBlock`'s `AdaLayerNorm` to adjust normalization parameters.

**Q: What image formats are supported?**  
A: All formats supported by PIL/Pillow (JPEG, PNG, etc.).

**Q: How to use LeRobot datasets?**  
A:
1. Install dependencies: `pip install lerobot datasets`
2. Prepare dataset (local path or HuggingFace dataset name)
3. Run training: `python train.py --config config.yaml --dataset_path ./dataset/libero_object`
4. Or configure in config.yaml: set `dataset.local_path` and related parameters

**Q: What is the model input format?**  
A: The project uses a unified dictionary format input:
```python
inputs = {
    "images": List[PIL.Image] or List[List[PIL.Image]],
    "instructions": List[str],
    "states": Optional[torch.Tensor],  # [B, state_dim]
    "actions": Optional[torch.Tensor]  # [B, action_horizon, action_dim]
}
```

**Q: How to download and test Qwen2-VL-2B-Instruct model?**  
A:
1. Download model: `python download_model.py --model Qwen/Qwen2-VL-2B-Instruct`
2. Run capability evaluation: `python test/evaluate_vlm_capabilities.py --model Qwen/Qwen2-VL-2B-Instruct`
3. Results are saved as JSON file, containing object recognition, spatial perception, causal reasoning test results.

**Q: What tests are included in VLM capability evaluation?**  
A: The evaluation script includes three types of tests:
- **Object Recognition**: Simple object recognition, color recognition, quantity counting
- **Spatial Perception**: Positional relationships, distance judgment, direction judgment
- **Causal Reasoning**: Action-result reasoning, scene understanding, logical reasoning

**Q: How to run tests?**  
A: The project includes a complete test suite:
```bash
# Run all tests
pytest test/

# Run specific tests
pytest test/test_vla_qwen_groot.py
pytest test/test_lerobot_training.py
```

**Q: What if state dimensions don't match?**  
A: The project implements automatic state dimension normalization. If you encounter dimension issues, check the normalization utilities in `src/ScriptedVLA/utils/normalization.py`.

### License

This project is open source under the MIT License.

### Contributing

Issues and Pull Requests are welcome!

### Related Documentation

- [VLM_EVALUATION.md](VLM_EVALUATION.md) - VLM Capability Evaluation Guide

### Acknowledgments

- [Qwen](https://github.com/QwenLM/Qwen-VL) - Vision Language Model
- [Transformers](https://github.com/huggingface/transformers) - Model Library
- [LeRobot](https://github.com/huggingface/lerobot) - Robot Learning Dataset Format
- [Flow Matching](https://arxiv.org/abs/2210.02747) - Flow Matching Architecture Inspiration

---

## ä¸­æ–‡ç‰ˆæœ¬

### é¡¹ç›®ç‰¹ç‚¹

- ğŸ¯ **ç®€å•æ˜“æ‡‚**ï¼šä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šè¯¦ç»†ï¼Œç»æ— è¿‡åº¦å°è£…ï¼Œé€‚åˆå­¦ä¹ å’Œç ”ç©¶
- ğŸ”§ **æ˜“äºé…ç½®**ï¼šä½¿ç”¨YAMLé…ç½®æ–‡ä»¶ï¼Œæ–¹ä¾¿è°ƒæ•´è¶…å‚æ•°
- ğŸš€ **å®Œæ•´æµç¨‹**ï¼šåŒ…å«æ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒã€æ¨ç†ç­‰å®Œæ•´åŠŸèƒ½
- ğŸ“¦ **ç°ä»£åŒ–å·¥å…·**ï¼šä½¿ç”¨uvè¿›è¡Œè™šæ‹Ÿç¯å¢ƒç®¡ç†
- ğŸ§© **æ¨¡å—åŒ–è®¾è®¡**ï¼šå„ç»„ä»¶ç‹¬ç«‹ï¼Œæ˜“äºæ‰©å±•å’Œä¿®æ”¹
- ğŸ¤– **LeRobotæ”¯æŒ**ï¼šåŸç”Ÿæ”¯æŒLeRobotæ•°æ®é›†æ ¼å¼ï¼ˆv2.1å’Œv3.0ï¼‰ï¼Œå…¼å®¹HuggingFaceå¼€æºæ•°æ®é›†
- ğŸ”„ **ç»Ÿä¸€æ¥å£**ï¼šç»Ÿä¸€çš„å­—å…¸æ ¼å¼è¾“å…¥ï¼Œè‡ªåŠ¨å¤„ç†çŠ¶æ€ç»´åº¦ï¼Œç®€åŒ–ä½¿ç”¨æµç¨‹
- ğŸ§ª **å®Œæ•´æµ‹è¯•**ï¼šåŒ…å«å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼Œç¡®ä¿ä»£ç è´¨é‡

### é¡¹ç›®ç»“æ„

```
ScriptedVLA/
â”œâ”€â”€ config.yaml                  # é…ç½®æ–‡ä»¶
â”œâ”€â”€ pyproject.toml               # é¡¹ç›®ä¾èµ–é…ç½®ï¼ˆuvï¼‰
â”œâ”€â”€ train.py                     # è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒLeRobotæ•°æ®é›†ï¼‰
â”œâ”€â”€ train_public_datasets.py     # å…¬å¼€æ•°æ®é›†è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference.py                 # æ¨ç†è„šæœ¬
â”œâ”€â”€ create_dummy_data.py         # åˆ›å»ºæµ‹è¯•æ•°æ®
â”œâ”€â”€ dataset_statistics.py        # æ•°æ®é›†ç»Ÿè®¡å’Œç­›é€‰å·¥å…·
â”œâ”€â”€ download_model.py            # æ¨¡å‹ä¸‹è½½è„šæœ¬
â”œâ”€â”€ analyze_state_dimensions.py  # çŠ¶æ€ç»´åº¦åˆ†æå·¥å…·
â”œâ”€â”€ README.md                    # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ VLM_EVALUATION.md            # VLMèƒ½åŠ›æµ‹è¯„æŒ‡å—
â”œâ”€â”€ test/                        # æµ‹è¯•ç›®å½•
â”‚   â”œâ”€â”€ test_vla_qwen_groot.py   # VLAæ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_vlm.py              # VLMæ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_action_head.py      # åŠ¨ä½œå¤´æµ‹è¯•
â”‚   â”œâ”€â”€ test_lerobot_training.py # LeRobotè®­ç»ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_lerobot_dataset_loader.py # LeRobotæ•°æ®åŠ è½½æµ‹è¯•
â”‚   â”œâ”€â”€ test_training.py         # è®­ç»ƒæµç¨‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_inference.py        # æ¨ç†æµ‹è¯•
â”‚   â””â”€â”€ evaluate_vlm_capabilities.py # VLMèƒ½åŠ›æµ‹è¯„è„šæœ¬
â””â”€â”€ src/
    â””â”€â”€ ScriptedVLA/            # PythonåŒ…ï¼ˆç¬¦åˆuvæ ‡å‡†ç»“æ„ï¼‰
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ model/               # æ¨¡å‹å®šä¹‰
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ vlm.py          # Qwen VLMæ¨¡å‹
        â”‚   â”œâ”€â”€ action_head.py  # Flow MatchingåŠ¨ä½œå¤´
        â”‚   â””â”€â”€ vla_qwen_groot.py  # Qwen-GR00T VLAæ¨¡å‹
        â”œâ”€â”€ data/                # æ•°æ®å¤„ç†
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ dataset.py      # è‡ªå®šä¹‰æ•°æ®é›†ç±»
        â”‚   â”œâ”€â”€ download_datasets.py # æ•°æ®é›†ä¸‹è½½å·¥å…·
        â”‚   â”œâ”€â”€ libero_dataset.py   # LIBEROæ•°æ®é›†é€‚é…å™¨
        â”‚   â”œâ”€â”€ act_dataset.py      # ACTæ•°æ®é›†é€‚é…å™¨
        â”‚   â””â”€â”€ lerobot_dataset_adapter.py # LeRobotæ•°æ®é›†é€‚é…å™¨
        â””â”€â”€ utils/               # å·¥å…·å‡½æ•°
            â”œâ”€â”€ __init__.py
            â”œâ”€â”€ config.py       # é…ç½®åŠ è½½
            â”œâ”€â”€ logger.py       # æ—¥å¿—å·¥å…·
            â””â”€â”€ normalization.py # çŠ¶æ€å½’ä¸€åŒ–å·¥å…·
```

### å¿«é€Ÿå¼€å§‹

#### 1. ç¯å¢ƒè®¾ç½®

ä½¿ç”¨uvåˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–ï¼š

```bash
# å®‰è£…uvï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
pip install uv

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
uv venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
uv pip install -e .
```

#### 2. å‡†å¤‡æ•°æ®

é¡¹ç›®ä¸»è¦æ”¯æŒ **LeRobotæ•°æ®é›†æ ¼å¼**ï¼ˆv2.1å’Œv3.0ï¼‰ï¼Œè¿™æ˜¯é»˜è®¤å’Œæ¨èçš„æ–¹å¼ã€‚

**ä½¿ç”¨LeRobotæ•°æ®é›†ï¼š**

```bash
# ä½¿ç”¨LeRobotæ•°æ®é›†è®­ç»ƒï¼ˆé»˜è®¤ä½¿ç”¨./dataset/libero_objectï¼‰
python train.py --config config.yaml

# æŒ‡å®šLeRobotæ•°æ®é›†è·¯å¾„
python train.py --config config.yaml --dataset_path ./dataset/libero_object

# ä½¿ç”¨HuggingFace LeRobotæ•°æ®é›†è®­ç»ƒ
# åœ¨config.yamlä¸­è®¾ç½®ï¼š
# dataset:
#   local_path: null
#   # æˆ–ç›´æ¥ä½¿ç”¨HFæ•°æ®é›†åç§°
```

**æ”¯æŒçš„LeRobotæ•°æ®é›†ï¼š**
- `lerobot/pusht`: PushTæ•°æ®é›†
- `k1000dai/libero-object-smolvla`: LIBERO Objectæ•°æ®é›†ï¼ˆLeRobotæ ¼å¼ï¼‰
- å…¶ä»–HuggingFaceä¸Šçš„LeRobotæ ¼å¼æ•°æ®é›†

#### 3. é…ç½®æ¨¡å‹

ç¼–è¾‘ `config.yaml` æ–‡ä»¶ï¼Œè°ƒæ•´æ¨¡å‹å’Œè®­ç»ƒå‚æ•°ï¼š

```yaml
model:
  vlm:
    model_name: "Qwen/Qwen2-VL-2B-Instruct"  # æ¨èæ¨¡å‹
    image_size: 448
    freeze_vlm: true  # å†»ç»“VLMå‚æ•°
  
  action_head:
    hidden_dim: 1536  # ä¸VLMè¾“å‡ºç»´åº¦åŒ¹é…
    num_layers: 6
    num_heads: 12
    action_dim: 7  # åŠ¨ä½œç»´åº¦
    action_horizon: 50  # åŠ¨ä½œåºåˆ—é•¿åº¦

dataset:
  local_path: "./dataset/libero_object"
  action_horizon: 50
  image_size: 224
  image_keys:
    - "observation.images.wrist_image"
  state_key: "observation.state"
  action_dim: 7

training:
  batch_size: 8
  num_epochs: 100
  learning_rate: 1e-4
  ...
```

#### 4. è®­ç»ƒæ¨¡å‹

```bash
# ä½¿ç”¨LeRobotæ•°æ®é›†è®­ç»ƒï¼ˆé»˜è®¤ï¼‰
python train.py --config config.yaml

# æŒ‡å®šæ•°æ®é›†è·¯å¾„
python train.py --config config.yaml --dataset_path ./dataset/libero_object

# è®¾ç½®è®­ç»ƒæ­¥æ•°å’Œä¿å­˜é—´éš”
python train.py --config config.yaml --max_steps 20000 --save_steps 5000

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python train.py --config config.yaml --resume ./checkpoints/checkpoint_epoch_50.pt
```

#### 5. ä¸‹è½½æ¨¡å‹

```bash
# ä¸‹è½½Qwen2-VL-2B-Instructæ¨¡å‹
python download_model.py --model Qwen/Qwen2-VL-2B-Instruct

# æˆ–ä¸‹è½½å…¶ä»–æ¨¡å‹
python download_model.py --model Qwen/Qwen-VL-Chat
```

#### 6. è¯„ä¼°VLMèƒ½åŠ›

```bash
# è¿è¡Œå®Œæ•´çš„æœºå™¨äººèƒ½åŠ›æµ‹è¯„
python test/evaluate_vlm_capabilities.py --model Qwen/Qwen2-VL-2B-Instruct

# ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹
python test/evaluate_vlm_capabilities.py --config config.yaml
```

æµ‹è¯„åŒ…æ‹¬ï¼š
- **ç‰©ä½“è¯†åˆ«èƒ½åŠ›**ï¼šè¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“ã€é¢œè‰²ã€æ•°é‡ç­‰
- **ç©ºé—´æ„ŸçŸ¥èƒ½åŠ›**ï¼šç†è§£ç‰©ä½“çš„ä½ç½®å…³ç³»ã€è·ç¦»ã€æ–¹å‘ç­‰
- **å› æœæ¨ç†èƒ½åŠ›**ï¼šæ ¹æ®å›¾æ–‡è¿›è¡ŒåŠ¨ä½œ-ç»“æœæ¨ç†ã€åœºæ™¯ç†è§£ã€é€»è¾‘æ¨ç†ç­‰

#### 7. æ¨ç†

```bash
python inference.py \
    --config config.yaml \
    --checkpoint ./checkpoints/best_model.pt \
    --image path/to/image.jpg \
    --text "Pick up the object"
```

### æ¨¡å‹æ¶æ„

#### VLMæ¨¡å—ï¼ˆQwenï¼‰
- åŸºäºQwen-VLæ¨¡å‹è¿›è¡Œè§†è§‰-è¯­è¨€ç†è§£
- å¤„ç†å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œè¾“å‡ºèåˆç‰¹å¾
- æ”¯æŒå†»ç»“VLMå‚æ•°ä»¥åŠ å¿«è®­ç»ƒ

#### åŠ¨ä½œå¤´ï¼ˆFlow Matchingï¼‰
- åŸºäºFlow Matchingæ¶æ„
- ä»VLMç‰¹å¾é¢„æµ‹æœºå™¨äººåŠ¨ä½œåºåˆ—ï¼ˆaction horizonï¼‰
- åŒ…å«å¤šå±‚Transformerï¼ˆDiT Blockï¼‰å’Œä½ç½®ç¼–ç 
- æ”¯æŒåŠ¨ä½œå—é¢„æµ‹ï¼ˆé¢„æµ‹æœªæ¥åŠ¨ä½œåºåˆ—ï¼‰
- **æ—¶é—´åµŒå…¥**ï¼šé€šè¿‡ `TimestepEncoder` å°†æ—¶é—´æ­¥ç¼–ç ä¸ºåµŒå…¥å‘é‡
- **è‡ªé€‚åº”å½’ä¸€åŒ–**ï¼šæ”¯æŒ `AdaLayerNorm`ï¼Œé€šè¿‡æ—¶é—´åµŒå…¥åŠ¨æ€è°ƒæ•´å½’ä¸€åŒ–å‚æ•°

#### å®Œæ•´VLAæ¨¡å‹
- ç»“åˆVLMå’ŒåŠ¨ä½œå¤´
- å¯é€‰äº¤å‰æ³¨æ„åŠ›æœºåˆ¶å¢å¼ºç‰¹å¾èåˆ
- ç«¯åˆ°ç«¯è®­ç»ƒ

### é…ç½®è¯´æ˜

#### æ¨¡å‹é…ç½®
- `vlm.model_name`: Qwenæ¨¡å‹åç§°
- `vlm.image_size`: è¾“å…¥å›¾åƒå°ºå¯¸ï¼ˆæ¨èï¼š448ï¼‰
- `vlm.freeze_vlm`: æ˜¯å¦å†»ç»“VLMå‚æ•°
- `action_head.hidden_dim`: Transformeréšè—å±‚ç»´åº¦ï¼ˆåº”ä¸VLMè¾“å‡ºåŒ¹é…ï¼‰
- `action_head.num_layers`: Transformerå±‚æ•°
- `action_head.num_heads`: æ³¨æ„åŠ›å¤´æ•°
- `action_head.action_dim`: åŠ¨ä½œç»´åº¦ï¼ˆå¦‚7ç»´ï¼šx, y, z, roll, pitch, yaw, gripperï¼‰
- `action_head.action_horizon`: åŠ¨ä½œåºåˆ—é•¿åº¦ï¼ˆchunkå¤§å°ï¼‰

#### è®­ç»ƒé…ç½®
- `batch_size`: æ‰¹æ¬¡å¤§å°
- `learning_rate`: å­¦ä¹ ç‡
- `num_epochs`: è®­ç»ƒè½®æ•°
- `max_steps`: æœ€å¤§è®­ç»ƒæ­¥æ•°
- `optimizer`: ä¼˜åŒ–å™¨é…ç½®
- `scheduler`: å­¦ä¹ ç‡è°ƒåº¦å™¨é…ç½®

#### æ•°æ®é›†é…ç½®
- `dataset.local_path`: æœ¬åœ°æ•°æ®é›†è·¯å¾„ï¼ˆLeRobotæ ¼å¼ï¼‰
- `dataset.action_horizon`: åŠ¨ä½œåºåˆ—é•¿åº¦
- `dataset.image_size`: å›¾åƒå°ºå¯¸
- `dataset.image_keys`: æ•°æ®é›†ä¸­çš„å›¾åƒé”®åï¼ˆä¾‹å¦‚ï¼š`["observation.images.wrist_image"]`ï¼‰
- `dataset.state_key`: æ•°æ®é›†ä¸­çš„çŠ¶æ€é”®å
- `dataset.action_dim`: åŠ¨ä½œç»´åº¦
- `dataset.task_description.use_batch_task`: ä»batchè·å–ä»»åŠ¡æè¿°ï¼ˆæ¨èï¼‰
- `dataset.task_description.use_tasks_jsonl`: ä»tasks.jsonlè·å–ä»»åŠ¡æè¿°ï¼ˆå¤‡é€‰ï¼‰

### LeRobotæ•°æ®é›†æ”¯æŒ

LeRobotæ˜¯HuggingFaceä¸Šçš„å¼€æºæœºå™¨äººå­¦ä¹ æ•°æ®é›†æ ¼å¼ï¼Œæ”¯æŒv2.1å’Œv3.0ç‰ˆæœ¬ã€‚é¡¹ç›®é»˜è®¤ä½¿ç”¨LeRobotæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚

**LeRobotæ•°æ®é›†ç‰¹ç‚¹ï¼š**
- æ”¯æŒParquetæ ¼å¼å­˜å‚¨ï¼ˆv3.0ï¼‰å’ŒHDF5æ ¼å¼ï¼ˆv2.1ï¼‰
- è‡ªåŠ¨ç‰ˆæœ¬æ£€æµ‹å’Œå…¼å®¹æ€§å¤„ç†
- æ”¯æŒaction chunkingï¼ˆåŠ¨ä½œåºåˆ—é¢„æµ‹ï¼‰
- åŒ…å«ä»»åŠ¡æè¿°å’Œå…ƒæ•°æ®

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```bash
# ä½¿ç”¨LeRobotæ•°æ®é›†è®­ç»ƒï¼ˆé»˜è®¤ï¼‰
python train.py --config config.yaml --dataset_path ./dataset/libero_object

# ä½¿ç”¨HuggingFaceæ•°æ®é›†
# åœ¨config.yamlä¸­ï¼Œè®¾ç½®dataset.local_pathä¸ºnullæˆ–ä½¿ç”¨HFæ•°æ®é›†åç§°
```

### åŒ…ç»“æ„è¯´æ˜

æœ¬é¡¹ç›®é‡‡ç”¨æ ‡å‡†çš„PythonåŒ…ç»“æ„ï¼Œç¬¦åˆuvå’Œç°ä»£PythonåŒ…ç®¡ç†å·¥å…·çš„è¦æ±‚ï¼š

- **åŒ…å**: `ScriptedVLA`ï¼ˆåœ¨ `src/ScriptedVLA/` ç›®å½•ä¸‹ï¼‰
- **å¯¼å…¥æ–¹å¼**: `from ScriptedVLA.model import ...`ï¼ˆå®‰è£…åå¯ç›´æ¥å¯¼å…¥ï¼‰
- **å®‰è£…æ–¹å¼**: `uv pip install -e .`ï¼ˆä»¥å¯ç¼–è¾‘æ¨¡å¼å®‰è£…ï¼Œä»£ç ä¿®æ”¹ç«‹å³ç”Ÿæ•ˆï¼‰

**ä¼˜åŠ¿ï¼š**
- âœ… ç¬¦åˆPEP 517/518æ ‡å‡†
- âœ… æ”¯æŒuvã€pipã€poetryç­‰ç°ä»£åŒ…ç®¡ç†å·¥å…·
- âœ… ä¾¿äºä»£ç ç»„ç»‡å’Œæ¨¡å—åŒ–
- âœ… æ”¯æŒä½œä¸ºåº“è¢«å…¶ä»–é¡¹ç›®å¼•ç”¨

### å¼€å‘è¯´æ˜

#### æ·»åŠ æ–°åŠŸèƒ½
1. **æ¨¡å‹æ‰©å±•**ï¼šåœ¨ `src/ScriptedVLA/model/` ä¸­æ·»åŠ æ–°æ¨¡å—
2. **æ•°æ®å¤„ç†**ï¼šåœ¨ `src/ScriptedVLA/data/` ä¸­æ·»åŠ æ•°æ®å¢å¼ºæˆ–æ–°æ•°æ®é›†
3. **å·¥å…·å‡½æ•°**ï¼šåœ¨ `src/ScriptedVLA/utils/` ä¸­æ·»åŠ è¾…åŠ©åŠŸèƒ½

#### å¯¼å…¥è¯´æ˜
æ‰€æœ‰è„šæœ¬æ–‡ä»¶ä½¿ç”¨ä»¥ä¸‹å¯¼å…¥æ–¹å¼ï¼š
```python
from ScriptedVLA.model import QwenGR00TVLAModel
from ScriptedVLA.data import VLADataset, LeRobotDatasetAdapter
from ScriptedVLA.utils import load_config, setup_logger, Normalizer
```

#### ä»£ç è§„èŒƒ
- ä½¿ç”¨ç±»å‹æç¤º
- æ·»åŠ æ–‡æ¡£å­—ç¬¦ä¸²
- ä¿æŒä»£ç ç®€æ´æ¸…æ™°

### æµ‹è¯•

é¡¹ç›®åŒ…å«å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest test/

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest test/test_vla_qwen_groot.py
pytest test/test_lerobot_training.py
pytest test/test_training.py
pytest test/test_inference.py
```

### å¸¸è§é—®é¢˜

**Q: å¦‚ä½•è°ƒæ•´åŠ¨ä½œç»´åº¦ï¼Ÿ**  
A: ä¿®æ”¹ `config.yaml` ä¸­çš„ `action_head.action_dim` å‚æ•°ã€‚

**Q: å¦‚ä½•å†»ç»“VLMå‚æ•°ï¼Ÿ**  
A: åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® `vlm.freeze_vlm: true`ã€‚

**Q: ä»€ä¹ˆæ˜¯ AdaLayerNormï¼Ÿå¦‚ä½•ä½¿ç”¨ï¼Ÿ**  
A: `AdaLayerNorm` æ˜¯ä¸€ç§è‡ªé€‚åº”å±‚å½’ä¸€åŒ–ï¼Œé€šè¿‡æ—¶é—´åµŒå…¥ï¼ˆtembï¼‰åŠ¨æ€è°ƒæ•´å½’ä¸€åŒ–å‚æ•°ã€‚è¿™å¯ä»¥è®©æ¨¡å‹æ ¹æ®æ—¶é—´æ­¥ä¿¡æ¯è‡ªé€‚åº”è°ƒæ•´å½’ä¸€åŒ–ï¼Œå¯èƒ½æé«˜Flow Matchingçš„æ€§èƒ½ã€‚

**Q: ä»€ä¹ˆæ˜¯æ—¶é—´åµŒå…¥ï¼ˆtembï¼‰ï¼Ÿ**  
A: æ—¶é—´åµŒå…¥æ˜¯å°†Flow Matchingä¸­çš„æ—¶é—´æ­¥ç¼–ç ä¸ºå‘é‡è¡¨ç¤ºï¼Œé€šè¿‡ `TimestepEncoder` å®ç°ã€‚å½“ä½¿ç”¨ `ada_norm` æ—¶ï¼Œæ—¶é—´åµŒå…¥ä¼šè¢«ä¼ é€’ç»™ `DiTBlock` çš„ `AdaLayerNorm`ï¼Œç”¨äºè°ƒæ•´å½’ä¸€åŒ–å‚æ•°ã€‚

**Q: æ”¯æŒå“ªäº›å›¾åƒæ ¼å¼ï¼Ÿ**  
A: æ”¯æŒPIL/Pillowæ”¯æŒçš„æ‰€æœ‰æ ¼å¼ï¼ˆJPEG, PNGç­‰ï¼‰ã€‚

**Q: å¦‚ä½•ä½¿ç”¨LeRobotæ•°æ®é›†ï¼Ÿ**  
A:
1. å®‰è£…ä¾èµ–ï¼š`pip install lerobot datasets`
2. å‡†å¤‡æ•°æ®é›†ï¼ˆæœ¬åœ°è·¯å¾„æˆ–HuggingFaceæ•°æ®é›†åç§°ï¼‰
3. è¿è¡Œè®­ç»ƒï¼š`python train.py --config config.yaml --dataset_path ./dataset/libero_object`
4. æˆ–åœ¨config.yamlä¸­é…ç½®ï¼šè®¾ç½® `dataset.local_path` å’Œç›¸å…³å‚æ•°

**Q: æ¨¡å‹çš„è¾“å…¥æ ¼å¼æ˜¯ä»€ä¹ˆï¼Ÿ**  
A: é¡¹ç›®ä½¿ç”¨ç»Ÿä¸€çš„å­—å…¸æ ¼å¼è¾“å…¥ï¼š
```python
inputs = {
    "images": List[PIL.Image] or List[List[PIL.Image]],
    "instructions": List[str],
    "states": Optional[torch.Tensor],  # [B, state_dim]
    "actions": Optional[torch.Tensor]  # [B, action_horizon, action_dim]
}
```

**Q: å¦‚ä½•ä¸‹è½½å’Œæµ‹è¯•Qwen2-VL-2B-Instructæ¨¡å‹ï¼Ÿ**  
A:
1. ä¸‹è½½æ¨¡å‹ï¼š`python download_model.py --model Qwen/Qwen2-VL-2B-Instruct`
2. è¿è¡Œèƒ½åŠ›æµ‹è¯„ï¼š`python test/evaluate_vlm_capabilities.py --model Qwen/Qwen2-VL-2B-Instruct`
3. ç»“æœä¼šä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼ŒåŒ…å«ç‰©ä½“è¯†åˆ«ã€ç©ºé—´æ„ŸçŸ¥ã€å› æœæ¨ç†ç­‰æµ‹è¯•ç»“æœã€‚

**Q: VLMèƒ½åŠ›æµ‹è¯„åŒ…å«å“ªäº›æµ‹è¯•ï¼Ÿ**  
A: æµ‹è¯„è„šæœ¬åŒ…å«ä¸‰ç±»æµ‹è¯•ï¼š
- **ç‰©ä½“è¯†åˆ«**ï¼šç®€å•ç‰©ä½“è¯†åˆ«ã€é¢œè‰²è¯†åˆ«ã€æ•°é‡ç»Ÿè®¡
- **ç©ºé—´æ„ŸçŸ¥**ï¼šä½ç½®å…³ç³»ã€è·ç¦»åˆ¤æ–­ã€æ–¹å‘åˆ¤æ–­
- **å› æœæ¨ç†**ï¼šåŠ¨ä½œ-ç»“æœæ¨ç†ã€åœºæ™¯ç†è§£ã€é€»è¾‘æ¨ç†

**Q: å¦‚ä½•è¿è¡Œæµ‹è¯•ï¼Ÿ**  
A: é¡¹ç›®åŒ…å«å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest test/

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest test/test_vla_qwen_groot.py
pytest test/test_lerobot_training.py
```

**Q: çŠ¶æ€ç»´åº¦ä¸åŒ¹é…æ€ä¹ˆåŠï¼Ÿ**  
A: é¡¹ç›®å·²å®ç°è‡ªåŠ¨çŠ¶æ€ç»´åº¦è§„èŒƒåŒ–ã€‚å¦‚æœé‡åˆ°ç»´åº¦é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ `src/ScriptedVLA/utils/normalization.py` ä¸­çš„å½’ä¸€åŒ–å·¥å…·ã€‚

## è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æºã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ç›¸å…³æ–‡æ¡£

- [VLM_EVALUATION.md](VLM_EVALUATION.md) - VLMèƒ½åŠ›æµ‹è¯„æŒ‡å—

## è‡´è°¢

- [Qwen](https://github.com/QwenLM/Qwen-VL) - è§†è§‰è¯­è¨€æ¨¡å‹
- [Transformers](https://github.com/huggingface/transformers) - æ¨¡å‹åº“
- [LeRobot](https://github.com/huggingface/lerobot) - æœºå™¨äººå­¦ä¹ æ•°æ®é›†æ ¼å¼
- [Flow Matching](https://arxiv.org/abs/2210.02747) - Flow Matchingæ¶æ„çµæ„Ÿ

