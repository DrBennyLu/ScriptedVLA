# VLA模型配置文件

# 模型配置
model:
  # VLM配置（Qwen）
  vlm:
    model_name: "Qwen/Qwen2-VL-2B-Instruct"  # 推荐使用Qwen2-VL-2B-Instruct，或 "Qwen/Qwen-VL-Chat"
    image_size: 448  # 推荐使用448以获得更好的视觉理解效果
    max_seq_length: 512
    freeze_vlm: true  # 是否冻结VLM参数
    cache_dir: "./cache/models"  # VLM模型本地缓存路径，如果为null则从HuggingFace下载
  
  # 动作头配置（Flow Matching）
  action_head:
    type: "flow_matching"  # Flow Matching动作头
    hidden_dim: 1536  # 与VLM输出维度匹配
    num_layers: 6
    num_heads: 12
    mlp_ratio: 4.0
    action_dim: 7  # 动作维度（例如：x, y, z, roll, pitch, yaw, gripper）
    # action_horizon: 动作序列长度（action chunk大小），即从当前时间步开始的未来N步动作
    # 注意：action_horizon = future_action_window_size + 1（包含当前动作）
    # 例如：future_action_window_size=10 时，action_horizon=11（当前动作+未来10步）
    action_horizon: 11  # 默认值，可以根据需要调整（实际值会从dataset配置中读取）
    num_inference_timesteps: 50  # Flow Matching推理步数
    patch_size: 16
    num_patches: 196  # 14x14 patches for 224x224 image
    
  # 完整VLA模型
  vla:
    use_cross_attention: true
    cross_attention_layers: 3
    # future_action_window_size: 未来动作窗口大小
    # 注意：action_horizon = future_action_window_size + 1
    # 如果action_horizon=11，则future_action_window_size=10
    future_action_window_size: 10  # 默认值，应该等于action_horizon - 1（实际值会从dataset配置中计算）

# 训练配置
training:
  batch_size: 8
  num_epochs: 100
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  
  # 优化器配置
  optimizer:
    type: "adamw"
    betas: [0.9, 0.999]
    eps: 1e-8
  
  # 学习率调度器
  scheduler:
    type: "cosine"
    warmup_ratio: 0.1
    min_lr_ratio: 0.01
  
  # 保存配置
  save_dir: "./checkpoints"
  save_steps: 50    #5000
  eval_steps: 50    #5000
  logging_steps: 10 #100
  
  # 最大训练步数（如果通过命令行指定，则优先使用命令行参数）
  max_steps: 100    #20000
  
  # 混合精度训练
  fp16: false
  bf16: false

# 数据集配置（LeRobot格式）
dataset:
  # 本地数据集路径（可以直接指定，也可以通过命令行参数 --dataset_path 覆盖）
  local_path: "./dataset/libero_object"
  
  # 数据集相关参数
  action_horizon: 50  # 动作序列长度（chunk大小）
  image_size: 224  # 图像尺寸
  
  # 相机配置（从数据集中读取的图像键名）
  # 单相机示例: ["observation.images.wrist_image"]
  # 多相机示例: ["observation.images.wrist_image", "observation.images.base_image"]
  image_keys:
    - "observation.images.image"
    - "observation.images.wrist_image"
  
  # 状态键名（从数据集中读取的状态键名）
  state_key: "observation.state"
  
  # 动作维度（从配置中直接指定，不再从数据集自动获取）
  action_dim: 7  # 例如：x, y, z, roll, pitch, yaw, gripper
  
  # 数据加载器配置
  dataloader:
    num_workers: 0  # 数据加载器工作进程数（Windows上建议使用0）
    pin_memory: false
    shuffle: true
  
  # 任务描述获取方式
  task_description:
    # 方式1: 直接从batch["task"]获取（推荐，如果数据集包含此字段）
    use_batch_task: true
    # 方式2: 从tasks.jsonl文件查询（如果use_batch_task为false或batch中没有task字段）
    use_tasks_jsonl: true  # 作为备选方案

# 数据配置（用于获取默认状态维度等）
data:
  # 机器人状态配置
  robot_state:
    # 状态维度（从配置中直接指定，不再从数据集自动获取）
    state_dim: 8
    # 是否使用状态信息
    use_state: true

# 推理配置
inference:
  checkpoint_path: "./checkpoints/best_model.pt"
  device: "cuda"  # 或 "cpu"
  batch_size: 1
  max_new_tokens: 50

# 日志配置
logging:
  use_wandb: false
  wandb_project: "vla-training"
  use_tensorboard: true
  log_dir: "./logs"

# 测试配置（用于test_training.py）
test:
  
  # 测试模型配置（可以使用较小的配置以加快测试）
  model:
    # VLM配置
    vlm:
      model_name: "Qwen/Qwen2-VL-2B-Instruct"
      cache_dir: "./cache/models"
      image_size: 224
    
    # 动作头配置（测试时使用较小的配置）
    action_head:
      hidden_dim: 768
      num_layers: 6  # 测试时使用较少的层数
      num_heads: 12
      action_dim: 7
      action_horizon: 50
      num_inference_timesteps: 10
    
    # VLA模型配置
    vla:
      use_state: true
      state_dim: 7
      future_action_window_size: 3
  
  # 测试训练配置
  training:
    # 训练轮数
    num_epochs: 2
    # 批次大小
    batch_size: 2
    # 学习率
    learning_rate: 1e-4
    # 权重衰减
    weight_decay: 0.01
    # 优化器配置
    optimizer:
      type: "adamw"
      betas: [0.9, 0.999]
      eps: 1e-8
    # 学习率调度器配置
    scheduler:
      type: "cosine"
      warmup_ratio: 0.1
      min_lr_ratio: 0.01
    # 梯度累积步数
    gradient_accumulation_steps: 1
    # 最大梯度范数
    max_grad_norm: 1.0
    # 日志记录步数
    logging_steps: 10
  
  # 测试数据加载器配置
  dataloader:
    # 数据加载器工作进程数（测试时使用0避免问题）
    num_workers: 0
    # 是否使用pin_memory
    pin_memory: false
    # 是否打乱数据
    shuffle: true
  
  # 数据集配置（用于创建测试数据集和VLADataset）
  dataset:
    # 每个数据集创建的episode数量
    num_episodes: 15
    # 每个episode的步数
    steps_per_episode: 50
    # 任务数量
    num_tasks: 1
    # 验证集episode数量（如果为null，则使用num_episodes // 2）
    val_episodes: null
    # action_horizon（动作序列长度）：从配置中的model.action_head.action_horizon获取
    # 如果未指定，则从test.model.action_head.action_horizon获取
    # 如果仍未指定，则使用默认值4
    action_horizon: null  # null表示从模型配置中获取
    # 如果episode末尾不够action_horizon，是否使用最后一个动作填充
    pad_action_chunk: true
  
  # 临时目录配置（如果为null，则自动创建临时目录）
  temp_dir: ./test_temp

# LeRobot训练测试配置（用于test_lerobot_training.py和test_inference.py）
lerobot_test:
  # 数据集配置
  dataset:
    # repo_id: HuggingFace数据集名称或本地数据集名称
    # 如果是本地路径，repo_id为目录名，root为父目录
    repo_id: "k1000dai/libero-object-smolvla"  # HF数据集名称
    # root: 本地数据集根目录（如果repo_id是本地路径，则使用此路径的父目录）
    root: null  # null表示使用HF数据集，或本地路径如"./dataset"
    # 本地数据集路径（如果使用本地路径，repo_id会被忽略）
    local_path: "./dataset/libero_object/"  # 本地数据集路径（可选）
    
    # action_horizon: 动作序列长度（chunk大小），默认50
    action_horizon: 50
    # 图像尺寸
    image_size: 224
  
  # 训练配置
  training:
    # 最大训练步数
    max_steps: 100
    # 批次大小
    batch_size: 2
    # 学习率（如果为null，从training配置中获取）
    learning_rate: null
    # 权重衰减（如果为null，从training配置中获取）
    weight_decay: null
    # 优化器配置（如果为null，从training配置中获取）
    optimizer: null
    # 学习率调度器配置（如果为null，从training配置中获取）
    scheduler: null
    # 梯度累积步数（如果为null，从training配置中获取）
    gradient_accumulation_steps: null
    # 最大梯度范数（如果为null，从training配置中获取）
    max_grad_norm: null
  
  # 数据加载器配置
  dataloader:
    num_workers: 0  # 测试时使用0避免问题
    pin_memory: false
    shuffle: true
  
  # 任务描述获取方式
  task_description:
    # 方式1: 直接从batch["task"]获取（推荐，如果数据集包含此字段）
    use_batch_task: true
    # 方式2: 从tasks.jsonl文件查询（如果use_batch_task为false或batch中没有task字段）
    use_tasks_jsonl: true  # 作为备选方案
